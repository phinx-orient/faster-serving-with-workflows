{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enqueued message: what is llama_index\n",
      "Enqueued message: how to use llama_index\n",
      "Enqueued message: give some example of llama_index\n",
      "Enqueued message: Message 4\n",
      "Enqueued message: Message 5\n",
      "Workflow 0 picked up message: what is llama_index\n",
      "Workflow 1 picked up message: how to use llama_index\n",
      "Workflow 2 picked up message: give some example of llama_index\n",
      "Workflow 0 completed processing message: what is llama_index\n",
      "Workflow 0 picked up message: Message 4\n",
      "Workflow 2 completed processing message: give some example of llama_index\n",
      "Workflow 2 picked up message: Message 5\n",
      "Workflow 1 completed processing message: how to use llama_index\n",
      "All workflows have completed their tasks.\n",
      "Workflow 0 completed processing message: Message 4\n",
      "All workflows have completed their tasks.\n",
      "Workflow 2 completed processing message: Message 5\n",
      "All workflows have completed their tasks.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from llama_index.core.workflow.handler import WorkflowHandler\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Define one single workflow\n",
    "class FirstEvent(Event):\n",
    "    first_output: str\n",
    "\n",
    "\n",
    "class SecondEvent(Event):\n",
    "    second_output: str\n",
    "    response: str\n",
    "\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ctx: Context, ev: StartEvent) -> FirstEvent:\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Step one is happening\"))\n",
    "        return FirstEvent(first_output=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ctx: Context, ev: FirstEvent) -> SecondEvent:\n",
    "        llm = AzureOpenAI(\n",
    "            engine=\"gpt-4o-mini\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=5000,\n",
    "        )\n",
    "        generator = await llm.astream_complete(\n",
    "            \"Please give me the first 3 paragraphs of Moby Dick, a book in the public domain.\"\n",
    "        )\n",
    "        async for response in generator:\n",
    "            # Allow the workflow to stream this piece of response\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=response.delta))\n",
    "        return SecondEvent(\n",
    "            second_output=\"Second step complete, full response attached\",\n",
    "            response=str(response),\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ctx: Context, ev: SecondEvent) -> StopEvent:\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Step three is happening\"))\n",
    "        return StopEvent(result=\"Workflow complete.\")\n",
    "\n",
    "\n",
    "# def to run workflow\n",
    "async def run_workflow(input_text, workflow):\n",
    "    \"\"\"Runs a single workflow instance.\"\"\"\n",
    "    handler: WorkflowHandler = workflow.run(first_input=input_text)\n",
    "\n",
    "    async for ev in handler.stream_events():\n",
    "        if isinstance(ev, ProgressEvent):  # Make sure ProgressEvent is defined/imported\n",
    "            # print(f\"Workflow for '{input_text}': {ev.msg}\")\n",
    "            pass\n",
    "    result = await handler\n",
    "    return result\n",
    "\n",
    "\n",
    "class WorkflowOrchestrator:\n",
    "    def __init__(self, num_workflows):\n",
    "        self.workflows = [\n",
    "            MyWorkflow(timeout=30, verbose=False) for i in range(num_workflows)\n",
    "        ]\n",
    "        self.message_queue = deque()\n",
    "        self.available_workflows = set(range(num_workflows))\n",
    "        self.total_processing_time = 0\n",
    "\n",
    "    async def enqueue_message(self, message):\n",
    "        self.message_queue.append(message)\n",
    "        print(f\"Enqueued message: {message}\")\n",
    "        if self.available_workflows:  # Wake up a waiting worker if available\n",
    "            self.available_workflows.pop()\n",
    "\n",
    "    async def process_messages(self, workflow_id):\n",
    "        while True:\n",
    "            if not self.message_queue:\n",
    "                self.available_workflows.add(workflow_id)\n",
    "                await asyncio.sleep(0)\n",
    "                if (\n",
    "                    workflow_id not in self.available_workflows\n",
    "                ):  # another process has grab the message and workflow_id not available now\n",
    "                    continue\n",
    "                elif not self.message_queue:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.available_workflows.pop()\n",
    "\n",
    "            message = self.message_queue.popleft()\n",
    "            print(f\"Workflow {workflow_id} picked up message: {message}\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = await run_workflow(message, self.workflows[workflow_id])\n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time  # Calculate processing time\n",
    "            self.total_processing_time += processing_time  # Sum processing time\n",
    "\n",
    "            print(f\"Workflow {workflow_id} completed processing message: {message}\")\n",
    "\n",
    "            # Check if all workflows are done\n",
    "            if not self.message_queue and not self.available_workflows:\n",
    "                print(\"All workflows have completed their tasks.\")\n",
    "                break\n",
    "\n",
    "    async def finish_processing(self):\n",
    "        return f\"Total processing time for all messages: {self.total_processing_time:.2f} seconds\"  # Print total time\n",
    "\n",
    "\n",
    "# async def main():\n",
    "\n",
    "\n",
    "num_workflows = 3\n",
    "orchestrator = WorkflowOrchestrator(num_workflows)\n",
    "\n",
    "# Create agent tasks\n",
    "agent_tasks = [orchestrator.process_messages(i) for i in range(num_workflows)]\n",
    "\n",
    "# Enqueue some messages\n",
    "messages = [\n",
    "    \"what is llama_index\",\n",
    "    \"how to use llama_index\",\n",
    "    \"give some example of llama_index\",\n",
    "    \"Message 4\",\n",
    "    \"Message 5\",\n",
    "]\n",
    "for message in messages:\n",
    "    await orchestrator.enqueue_message(message)\n",
    "\n",
    "await asyncio.gather(\n",
    "    *agent_tasks\n",
    ")  # keep the program running even when there is no message\n",
    "processing_time = await orchestrator.finish_processing()\n",
    "# Ensure we finish processing after all tasks are done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Example cover:\n",
    "- How to defind a single Workflow of Llama index\n",
    "- Run Workflow in async mode\n",
    "- Create a Orchetrator that can handle the incomming messages and assign to idle agents (workflow)\n",
    "- Run with multiples workflows (agents) to reduce the latency for 5 input messages at the same time\n",
    "    + 1 agent: 18s running\n",
    "    + 2 agents: 11s running (reduce up to 38% time execution)\n",
    "    + 3 agents: 9s running (reduce up to 50% time execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
